{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Geeks for Geeks - Naive Bayes Classifiers](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "[Geeks for Geeks - Multinomial Naive Bayes](https://www.geeksforgeeks.org/multinomial-naive-bayes/)\n",
    "\n",
    "[Geeks for Geeks - Gaussian Naive Bayes](https://www.geeksforgeeks.org/gaussian-naive-bayes/)\n",
    "\n",
    "[Geeks for Geeks - Bernoulli Naive Bayes](https://www.geeksforgeeks.org/bernoulli-naive-bayes/)\n",
    "\n",
    "[Scikit Learn - Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Characteristics\n",
    "- Supervised Learning\n",
    "- Classification\n",
    "- Based on Bayes' Theorem and conditional probability\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Assumptions\n",
    "- Conditional independence among features\n",
    "    - Given label, all features are independent\n",
    "- Features follow specific distributions\n",
    "    - Gaussian, Multinomial, or Bernoulli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input & Output\n",
    "- **Input**: features $X$ \n",
    "- **Output**: predicted label $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- $P(y)$ for all labels $y$\n",
    "- $P(x\\mid y)$ for all labels $y$ and feature value $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "- **Training**: $O(d)$ per sample\n",
    "- **Inference**: $O(d)$\n",
    "\n",
    "where $d$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons\n",
    "- **Advantages**: \n",
    "    - small parameter size\n",
    "    - fast inference\n",
    "    - performs well on categorical features\n",
    "* **Disadvantages**: \n",
    "    - assumption for independence does not always hold in real world\n",
    "    - poor generalization for unseen events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "- spam filtering\n",
    "- sentiment analysis\n",
    "- classifying texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$$P(y\\mid X) = \\frac{P(X\\mid y) P(y)}{P(X)}=\\frac{P(x_1,x_2,\\ldots,x_n \\mid y)P(y)}{P(x_1,x_2,\\ldots,x_n)}$$\n",
    "\n",
    "- $P(y \\mid X)$: posterior probability of label $y$ given features $X$\n",
    "- $P(X \\mid y)$: likelihood of features $X$ given label $y$\n",
    "- $P(y)$: prior probability of label $y$\n",
    "- $P(X)$: probability of features $X$\n",
    "\n",
    "With conditional independence between $x_1, x_2, \\ldots, x_d$,\n",
    "$$P(y\\mid x_1, \\ldots x_n) = \\frac{P(y)P(x_1\\mid y)P(x_2\\mid y)\\cdots P(x_n\\mid y)}{P(x_1)P(x_2)\\cdots P(x_n)} = \\frac{P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)}{P(x_1)P(x_2)\\cdots P(x_n)}$$\n",
    "\n",
    "Since we want to find the label with the highest posterior probability given input features, \n",
    "$$P(y\\mid x_1, \\ldots x_n)\\propto P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)$$\n",
    "$$\\hat{y}=\\arg\\max_{y}\\bigg(P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)\\bigg)$$\n",
    "\n",
    "For all three Naive Bayes models below, we use:\n",
    "$$P(y)=\\frac{N_y}{n}$$\n",
    "where\n",
    "- $N_y$: occurence of class $y$\n",
    "- $n$: number of data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Used when features represent counts or frequencies\n",
    "- text classification\n",
    "\n",
    "$$P(x_i\\mid y) = \\frac{N_{x_i,y}+1}{N_y+V}$$\n",
    "\n",
    "where \n",
    "- $N_{x_i,y}$: count of word $x_i$ in label $y$\n",
    "- $N_{y}$: total count of words in label $y$\n",
    "- $V$: vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Used for continuous numerical features\n",
    "\n",
    "$$P(x_i\\mid y)=\\frac{1}{\\sqrt{2\\pi \\sigma_y^2}}e^{-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2}}$$\n",
    "\n",
    "where\n",
    "- $\\mu_y$: mean of feature $x_i$ for label $y$\n",
    "- $\\sigma_y$: variance of feature $x_i$ for label $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes\n",
    "\n",
    "Used for binary features\n",
    "- presence or absence in text classification\n",
    "\n",
    "$$P(x_i\\mid y) = P(i\\mid y)(x_i) + (1-P(i\\mid y))(1-x_i)=\\begin{cases}P(i\\mid y)&x_i=1\\\\1-P(i\\mid y)&x_i=0\\end{cases}$$\n",
    "\n",
    "where\n",
    "- $P(i\\mid y)$: probability that event $i$ happens given label $y$\n",
    "- $x_i$: indicator variable of event $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comments"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

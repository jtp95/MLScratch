{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Geeks for Geeks - Naive Bayes Classifiers](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "[Geeks for Geeks - Multinomial Naive Bayes](https://www.geeksforgeeks.org/multinomial-naive-bayes/)\n",
    "\n",
    "[Geeks for Geeks - Gaussian Naive Bayes](https://www.geeksforgeeks.org/gaussian-naive-bayes/)\n",
    "\n",
    "[Geeks for Geeks - Bernoulli Naive Bayes](https://www.geeksforgeeks.org/bernoulli-naive-bayes/)\n",
    "\n",
    "[Scikit Learn - Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Characteristics\n",
    "- Supervised Learning\n",
    "- Classification\n",
    "- Based on Bayes' Theorem and conditional probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Assumptions\n",
    "- Conditional independence among features\n",
    "    - Given label, all features are independent\n",
    "- Features follow specific distributions\n",
    "    - Gaussian, Multinomial, or Bernoulli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input & Output\n",
    "- **Input**: features $X$ \n",
    "    - Multinomial NB: matrix where $X[j][i]$ is occurence of $i$-th feature in $j$-th sample\n",
    "    - Gaussian NB: matrix where $X[j][i]$ is a real number representing $i$-th feature (word) in $j$-th sample\n",
    "    - Bernoulli NB: matrix where $X[j][i]$ is indicator ($1$ or $0$) of whether $i$-th feature is present in $j$-th sample\n",
    "- **Output**: predicted label $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- $P(y)$ for all labels $y$\n",
    "- $P(x\\mid y)$ for all labels $y$ and feature value $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "- **Training**: $O(d)$ per sample\n",
    "- **Inference**: $O(d)$\n",
    "\n",
    "where $d$ is the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons\n",
    "- **Advantages**: \n",
    "    - small parameter size\n",
    "    - fast inference\n",
    "    - performs well on categorical features\n",
    "* **Disadvantages**: \n",
    "    - assumption for independence does not always hold in real world\n",
    "    - poor generalization for unseen events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "- spam filtering\n",
    "- sentiment analysis\n",
    "- classifying texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "$$P(y\\mid X) = \\frac{P(X\\mid y) P(y)}{P(X)}=\\frac{P(x_1,x_2,\\ldots,x_n \\mid y)P(y)}{P(x_1,x_2,\\ldots,x_n)}$$\n",
    "\n",
    "- $P(y \\mid X)$: posterior probability of label $y$ given features $X$\n",
    "- $P(X \\mid y)$: likelihood of features $X$ given label $y$\n",
    "- $P(y)$: prior probability of label $y$\n",
    "- $P(X)$: probability of features $X$\n",
    "\n",
    "With conditional independence between $x_1, x_2, \\ldots, x_d$,\n",
    "$$P(y\\mid x_1, \\ldots x_n) = \\frac{P(y)P(x_1\\mid y)P(x_2\\mid y)\\cdots P(x_n\\mid y)}{P(x_1)P(x_2)\\cdots P(x_n)} = \\frac{P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)}{P(x_1)P(x_2)\\cdots P(x_n)}$$\n",
    "\n",
    "Since we want to find the label with the highest posterior probability given input features, \n",
    "$$P(y\\mid x_1, \\ldots x_n)\\propto P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)$$\n",
    "$$\\hat{y}=\\arg\\max_{y}\\bigg(P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)\\bigg)$$\n",
    "\n",
    "However, as probability ranges between $0$ and $1$, the product of multiple probabilities can be too small.\\\n",
    "So, we can take use logarithmic function (strictly increasing) to make numbers manageable:\n",
    "$$\\log\\bigg(P(y\\mid x_1, \\ldots x_n)\\bigg)\\propto \\log\\bigg(P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)\\bigg) = \\log(P(y))+\\sum\\limits_{i=1}^{n}\\log(P(x_i\\mid y))$$ \n",
    "$$\\hat{y}=\\arg\\max_{y}\\bigg(\\log(P(y))+\\sum\\limits_{i=1}^{n}\\log(P(x_i\\mid y))\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior\n",
    "For all three Naive Bayes models below, we use:\n",
    "$$P(y)=\\frac{N_y}{n}$$\n",
    "where\n",
    "- $N_y$: occurence of class $y$\n",
    "- $n$: number of data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Multinomial Naive Bayes\n",
    "\n",
    "Used when features represent counts or frequencies\n",
    "- text classification\n",
    "\n",
    "$$P(x_i\\mid y) = \\frac{N_{x_i,y}+1}{N_y+V}$$\n",
    "\n",
    "where \n",
    "- $N_{x_i,y}$: count of word $x_i$ in label $y$\n",
    "- $N_{y}$: total count of words in label $y$\n",
    "- $V$: vocabulary size\n",
    "\n",
    "We use $P(x_i\\mid y)^{x_i}$ instead of $P(x_i\\mid y)$ to ensures that frequent words have a greater effect on probability estimates.\n",
    "\n",
    "$$P(y\\mid x_1, \\ldots x_n)\\propto P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y)^{x_i}= \\log(P(y))+\\sum\\limits_{i=1}^{n}x_i\\cdot\\log(P(x_i\\mid y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Used for continuous numerical features\n",
    "\n",
    "$$P(x_i\\mid y)=\\frac{1}{\\sqrt{2\\pi \\sigma_{y,i}^2}}e^{-\\frac{(x_i-\\mu_{y,i})^2}{2\\sigma_{y,i}^2}}$$\n",
    "\n",
    "where\n",
    "- $\\mu_{y,i}$: mean of feature $x_i$ for label $y$\n",
    "- $\\sigma_{y,i}^2$: variance of feature $x_i$ for label $y$\n",
    "\n",
    "$$P(y\\mid x_1, \\ldots x_n)\\propto P(y)\\prod\\limits_{i=1}^{n}P(x_i\\mid y) = \\log(P(y))+\\sum\\limits_{i=1}^{n}\\log(P(x_i\\mid y))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli Naive Bayes\n",
    "\n",
    "Used for binary features\n",
    "- presence or absence in text classification\n",
    "\n",
    "$$P(x_i\\mid y) = P(i\\mid y)(x_i) + (1-P(i\\mid y))(1-x_i)=\\begin{cases}P(i\\mid y)&x_i=1\\\\1-P(i\\mid y)&x_i=0\\end{cases}$$\n",
    "\n",
    "where\n",
    "- $P(i\\mid y)$: probability that event $i$ happens given label $y$\n",
    "- $x_i$: indicator variable of event $i$\n",
    "\n",
    "We use $P(x_i | y)^{x_i} (1 - P(x_i | y))^{(1 - x_i)}$ instead of $P(x_i | y)$ to ensure the absence of word also contribute to the probability.\n",
    "\n",
    "$$P(y\\mid x_1, \\ldots x_n)\\propto P(y)\\prod\\limits_{i=1}^{n}\\bigg(P(x_i | y)^{x_i} (1 - P(x_i | y))^{(1 - x_i)}\\bigg)= \\log(P(y))+\\sum\\limits_{i=1}^{n}\\bigg(x_i\\log P(x_i | y)+ (1-x_i) \\log(1 - P(x_i | y)) \\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli NB model will not be implemented"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit Learn - Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "[Geeks for Geeks - Decision Tree](https://www.geeksforgeeks.org/decision-tree/)\n",
    "\n",
    "[Geeks for Geeks - Decision Tree in Machine Learning](https://www.geeksforgeeks.org/decision-tree-introduction-example/)\n",
    "\n",
    "[Geeks for Geeks - Python | Decision tree implementation](https://www.geeksforgeeks.org/decision-tree-implementation-python/)\n",
    "\n",
    "[Geeks for Geeks - Python | Decision Tree Regression using sklearn](https://www.geeksforgeeks.org/python-decision-tree-regression-using-sklearn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "- Supervised Learning\n",
    "- Classification and Regression\n",
    "- Uses tree like structure where\n",
    "    - internal nodes are questions\n",
    "    - branches are paths for each answer\n",
    "    - leaf nodes are predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input & Output\n",
    "- **Input**: feature matrix $X$ with shape (n_samples, n_features)\n",
    "- **Output**: label or target value vector $y$ with shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- splitting feature\n",
    "- splitting threshold\n",
    "- tree structure\n",
    "- leaf predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- maximum depth\n",
    "- minimum samples for node (internal and leaf)\n",
    "- splitting metric\n",
    "    - gini index\n",
    "    - entropy\n",
    "    - mean squared error\n",
    "- minimum gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "- **Training**: $O(n\\log(n) \\cdot m\\cdot d)$\n",
    "    - for each feature, we sort ($O(n\\log n)$) and calculate gain for each of $n-1$ thresholds\n",
    "    - so, for each level, it takes $O(n\\log n\\cdot m)$\n",
    "- **Inference**: $O(d)$\n",
    "\n",
    "where \n",
    "- $n$: number of samples\n",
    "- $m$: number of features\n",
    "- $d$: depth of the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons\n",
    "- **Pros**: \n",
    "    - easy to understand and visualize the model\n",
    "    - decision boundaries are not linear and can be more complex\n",
    "    - little data pre processing is required\n",
    "    - fast inference\n",
    "\n",
    "* **Cons**: \n",
    "    - high risk for overfitting\n",
    "    - not optimal since we decide boundary greedily\n",
    "    - biased towards features with more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Internal / Root Nodes\n",
    "\n",
    "When creating an internal node, we receive a set of samples $S$.\n",
    "\n",
    "For each feature $f_i$:\n",
    "- Sort $S$ based on the values of feature $f_i$\n",
    "- Evaluate potential thresholds or intervals that split $S$ and compute the corresponding gain\n",
    "\n",
    "Select the splitting feature $f$ and partition $V_f$ that yields the maximum gain.\n",
    "\n",
    "Create new child nodes using subsets $S_v\\subseteq S$, where each $S_v=\\{x \\in S \\mid x[f] \\in v\\}$ for $v \\in V_f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Leaf Nodes\n",
    "\n",
    "When creating a leaf node, we receive a set of samples $S$.\n",
    "\n",
    "- Classification: the prediction is the most frequent class in $S$.\n",
    "- Regression: the prediction is the mean of the target values in $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain\n",
    "\n",
    "$$\\text{Gain}(S,V_f,f) = i(S) - \\sum_{v\\in V_f}\\frac{|S_v|}{|S|}i(S_v)$$\n",
    "\n",
    "where \n",
    "- $S$: set of samples in the node\n",
    "- $f$: index of splitting feature\n",
    "- $V_f$: set of intervals representing partition on feature $f$\n",
    "- $S_v$: subset of $S$ such that $x[f]\\in v$ \n",
    "- $i(S)$: impurity of set $S$ (e.g., Gini, Entropy, MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain (Binary Trees)\n",
    "\n",
    "$$\\text{Gain}(S,t,f) = i(S) - \\frac{|L|}{|S|}i(L) - \\frac{|R|}{|S|}i(R)$$\n",
    "$$L=\\{x\\in S\\mid x[f]\\leq t\\},\\quad R=\\{x\\in S\\mid x[f]> t\\}$$\n",
    "\n",
    "where \n",
    "- $S$: set of samples in the node\n",
    "- $t$: splitting threshold\n",
    "- $f$: splitting feature\n",
    "- $i(S)$: impurity of set $S$ (e.g., Gini, Entropy, MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Index\n",
    "$$\\text{Gini Index} = 1 - \\sum_{i\\in C}(p_i)^2$$\n",
    "\n",
    "where \n",
    "- $C$: set of classes\n",
    "- $p_i$: proportion of samples with class $i$ in the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n",
    "$$H = -\\sum_{i\\in C}p_i\\log_2(p_i)$$\n",
    "\n",
    "where \n",
    "- $C$: set of classes\n",
    "- $p_i$: proportion of samples with class $i$ in the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\overline{y})^2$$\n",
    "\n",
    "where \n",
    "- $n$: number of samples in the node\n",
    "- $y_i$: target value of $i$-th sample in the node\n",
    "- $\\overline{y}$: mean target value of the samples in the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only binary decision tree will be implemented"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

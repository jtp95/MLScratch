{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit Learn - Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "[IBM - What are support vector machines (SVMs)?](https://www.ibm.com/think/topics/support-vector-machine)\n",
    "\n",
    "[Medium - Math behind SVM (Support Vector Machine)](https://ankitnitjsr13.medium.com/math-behind-support-vector-machine-svm-5e7376d0ee4d)\n",
    "\n",
    "[Spot Intelligence - Support Vector Machines (SVM) In Machine Learning Made Simple & How To Tutorial](https://spotintelligence.com/2024/05/06/support-vector-machines-svm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "- Supervised learning\n",
    "- usually classification task\n",
    "- finds best (maximum margin) hyperplane that separates classes\n",
    "- margin is the hyperplane and nearest data points (support vectors)\n",
    "- larger margin means better generalization usually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input & Output\n",
    "- **Input**: feature matrix $X$ with shape (n_samples, n_features)\n",
    "- **Output**: lable vector $y$ with shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- $\\vec{w}$: weight vector of hyperplane\n",
    "- $b$: bias term of hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- $C$: regularization rate\n",
    "- $\\alpha$: learning rate\n",
    "- number of epochs\n",
    "- batch size\n",
    "- kernel type & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "For linear SVM,\n",
    "- **Training**: $O(n\\cdot d)$ per epoch\n",
    "    - update parameters for each data during optimization\n",
    "* **Inference**: $O(d)$\n",
    "    - get sign of signed distance from hyperplane to the data point\n",
    "\n",
    "where\n",
    "- $n$: number of training data\n",
    "- $d$: number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons\n",
    "Linear SVM:\n",
    "- **Pros**:\n",
    "    - fast training\n",
    "    - scales well to large datasets\n",
    "    - simple mathematics\n",
    "    - efficient inference\n",
    "* **Cons**: \n",
    "    - works well only if data is linearly separable\n",
    "\n",
    "\n",
    "Polynomial Kernel SVM:\n",
    "- **Pros**:\n",
    "    - captures non linear relationship\n",
    "    - fast on small datasets\n",
    "- **Cons**:\n",
    "    - more parameters\n",
    "    - computationally heavier than linear SVMs\n",
    "    - might overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "- $\\vec{w}$: weight vector\n",
    "- $\\vec{x}$: data point\n",
    "- $b$: bias term\n",
    "- $y$: label ($1$ or $-1$)\n",
    "- $\\mu$: margin ($\\frac{1}{\\|\\vec{w}\\|}$) \n",
    "- $C$: regularization rate\n",
    "- $N$: number of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary\n",
    "$$P:\\vec{w}\\cdot\\vec{x}+b=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signed Distance\n",
    "Signed distance represents which side and how far $\\vec{x}$ is from the hyperplane $P$\n",
    "$$d(\\vec{x})=\\frac{\\vec{w}\\cdot\\vec{x}+b}{\\|\\vec{w}\\|}$$\n",
    "\n",
    "- $d(\\vec{x})=0$ means $\\vec{x}$ is on the hyperplane\n",
    "- $0<|d(\\vec{x})|\\leq 1$ means $\\vec{x}$ is within the margin ($\\mu=\\frac{1}{\\|\\vec{w}\\|}$) from the hyperplane\n",
    "- $1< |d(\\vec{x})|$ means $\\vec{x}$ is more than the margin away from the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "Predicted label is the sign ($+1$ or $-1$) of the signed distance from the decision boundary.\n",
    "$$\\hat{y}=\\text{sign}(\\vec{w}\\cdot\\vec{x}+b)$$\n",
    "\n",
    "The prediction is correct if $\\hat{y}$ and $y$ are both $+1$'s or both $-1$'s. \\\n",
    "Similarly, the precition will be correct if $d(\\vec{x})$ and $y$ have the same sign:\n",
    "$$y\\cdot d(\\vec{x})>0 \\quad \\text{or} \\quad y(\\vec{w}\\cdot\\vec{x}+b)>0$$\n",
    "\n",
    "- $y(\\vec{w}\\cdot\\vec{x}+b)<0$ means $\\vec{x}$ is incorrectly predicted\n",
    "- $0\\leq y(\\vec{w}\\cdot\\vec{x}+b)\\leq 1$ means $\\vec{x}$ is correctly predicted but within the margin\n",
    "- $1< y(\\vec{w}\\cdot\\vec{x}+b)$ means $\\vec{x}$ is correctly predicted and far enough from the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vectors\n",
    "Support vectors are data points that are incorrectly classified or correctly classified but within margin distance from the decision boundary.\n",
    "$$\\text{SV}=\\{\\vec{x}_i \\mid y_i(\\vec{w}\\cdot\\vec{x}_i+b)\\leq 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge Loss\n",
    "$$\\ell(\\vec{x},y)=\\max\\big(0, 1-y(\\vec{w}\\cdot\\vec{x}+b)\\big)$$\n",
    "\n",
    "- if $\\vec{x}$ is incorrectly predicted, $\\ell(\\vec{x},y) > 1$\n",
    "- if $\\vec{x}$ is correctly predicted but within the margin, $1 \\geq \\ell(\\vec{x},y) > 0$\n",
    "- if $\\vec{x}$ is correctly predicted and far enough from the boundary, $\\ell(\\vec{x},y) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Risk\n",
    "$$L(\\vec{w},b)=\\frac{1}{2}\\|\\vec{w}\\|^2+\\frac{C}{N}\\sum_{i=1}^{N}\\ell(\\vec{x}_i,y_i)$$\n",
    "$$L_{\\text{SGD}}(\\vec{w},b)=\\frac{1}{2}\\|\\vec{w}\\|^2+C\\cdot\\ell(\\vec{x}_i,y_i)$$\n",
    "\n",
    "- $\\frac{1}{2}\\|\\vec{w}\\|^2$: regularization term; controls (increases) margin size\n",
    "- $C$: regularization term; manages trade off between margin size and classification error\n",
    "- $\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\vec{x}_i,y_i)$: classification error; sum of losses for all data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "$$\\frac{\\partial \\ell}{\\partial \\vec{w}} = \\begin{cases}0&\\vec{x}\\not\\in\\text{SV}\\\\-y\\cdot\\vec{x}&\\vec{x}\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L}{\\partial \\vec{w}}=\\vec{w}+\\frac{C}{N}\\sum_{i=1}^{N}\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-y_i\\cdot\\vec{x}_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L_{\\text{SGD}}}{\\partial \\vec{w}}=\\vec{w}+\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-C\\cdot y_i\\cdot\\vec{x}_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial b} = \\begin{cases}0&\\vec{x}\\not\\in\\text{SV}\\\\-y&\\vec{x}\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L}{\\partial b}=\\frac{C}{N}\\sum_{i=1}^{N}\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-y_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L_{\\text{SGD}}}{\\partial b}=\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-C\\cdot y_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update\n",
    "We will use Stochastic Gradient Descent (SGD)\n",
    "\n",
    "If $\\vec{x}$ is a support vector:\n",
    "$$\\vec{w} \\gets \\vec{w} - \\alpha\\cdot\\vec{w}+\\bigg(\\alpha\\cdot C\\cdot y_i\\cdot \\vec{x}_i\\bigg)$$\n",
    "$$b \\gets b + \\bigg(\\alpha\\cdot C\\cdot y_i\\bigg)$$\n",
    "\n",
    "If $\\vec{x}$ is not a support vector:\n",
    "$$\\vec{w} \\gets \\vec{w} - \\alpha\\cdot\\vec{w}$$\n",
    "$$b \\gets b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Classification SVMs (linear and polynomial kernel) will be implemented."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

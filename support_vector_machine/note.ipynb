{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit Learn - Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "[IBM - What are support vector machines (SVMs)?](https://www.ibm.com/think/topics/support-vector-machine)\n",
    "\n",
    "[Medium - Math behind SVM (Support Vector Machine)](https://ankitnitjsr13.medium.com/math-behind-support-vector-machine-svm-5e7376d0ee4d)\n",
    "\n",
    "[Spot Intelligence - Support Vector Machines (SVM) In Machine Learning Made Simple & How To Tutorial](https://spotintelligence.com/2024/05/06/support-vector-machines-svm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "- Supervised learning\n",
    "- usually classification task\n",
    "- finds best (maximum margin) hyperplane that separates classes\n",
    "- margin is the hyperplane and nearest data points (support vectors)\n",
    "- larger margin means better generalization usually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input & Output\n",
    "- **Input**: feature matrix $X$ with shape (n_samples, n_features)\n",
    "- **Output**: lable vector $y$ with shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "- $\\vec{w}$: weight vector of hyperplane\n",
    "- $b$: bias term of hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- $C$: regularization rate\n",
    "- $\\alpha$: learning rate\n",
    "- number of epochs\n",
    "- batch size\n",
    "- kernel type & parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime Complexity\n",
    "For linear SVM,\n",
    "- **Training**: $O(n\\cdot d)$ per epoch\n",
    "    - update parameters for each data during optimization\n",
    "* **Inference**: $O(d)$\n",
    "    - get sign of signed distance from hyperplane to the data point\n",
    "\n",
    "where\n",
    "- $n$: number of training data\n",
    "- $d$: number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons\n",
    "Linear SVM:\n",
    "- **Pros**:\n",
    "    - fast training\n",
    "    - scales well to large datasets\n",
    "    - simple mathematics\n",
    "    - efficient inference\n",
    "* **Cons**: \n",
    "    - works well only if data is linearly separable\n",
    "\n",
    "\n",
    "Kernel SVM:\n",
    "- **Pros**:\n",
    "    - captures non linear relationship\n",
    "    - fast on small datasets\n",
    "- **Cons**:\n",
    "    - more parameters\n",
    "    - computationally heavier than linear SVMs\n",
    "    - might overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "- $\\vec{w}$: weight vector\n",
    "- $\\vec{x}$: data point\n",
    "- $b$: bias term\n",
    "- $y$: label ($1$ or $-1$)\n",
    "- $\\mu$: margin ($\\frac{1}{\\|\\vec{w}\\|}$) \n",
    "- $C$: regularization rate\n",
    "- $N$, $n$: number of data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary\n",
    "$$P:\\vec{w}\\cdot\\vec{x}+b=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signed Distance\n",
    "Signed distance represents which side and how far $\\vec{x}$ is from the hyperplane $P$\n",
    "$$d(\\vec{x})=\\frac{\\vec{w}\\cdot\\vec{x}+b}{\\|\\vec{w}\\|}$$\n",
    "\n",
    "- $d(\\vec{x})=0$ means $\\vec{x}$ is on the hyperplane\n",
    "- $0<|d(\\vec{x})|\\leq 1$ means $\\vec{x}$ is within the margin ($\\mu=\\frac{1}{\\|\\vec{w}\\|}$) from the hyperplane\n",
    "- $1< |d(\\vec{x})|$ means $\\vec{x}$ is more than the margin away from the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "Predicted label is the sign ($+1$ or $-1$) of the signed distance from the decision boundary.\n",
    "$$\\hat{y}=\\text{sign}(\\vec{w}\\cdot\\vec{x}+b)$$\n",
    "\n",
    "The prediction is correct if $\\hat{y}$ and $y$ are both $+1$'s or both $-1$'s. \\\n",
    "Similarly, the precition will be correct if $d(\\vec{x})$ and $y$ have the same sign:\n",
    "$$y\\cdot d(\\vec{x})>0 \\quad \\text{or} \\quad y(\\vec{w}\\cdot\\vec{x}+b)>0$$\n",
    "\n",
    "- $y(\\vec{w}\\cdot\\vec{x}+b)<0$ means $\\vec{x}$ is incorrectly predicted\n",
    "- $0\\leq y(\\vec{w}\\cdot\\vec{x}+b)\\leq 1$ means $\\vec{x}$ is correctly predicted but within the margin\n",
    "- $1< y(\\vec{w}\\cdot\\vec{x}+b)$ means $\\vec{x}$ is correctly predicted and far enough from the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vectors\n",
    "Support vectors are data points that are incorrectly classified or correctly classified but within margin distance from the decision boundary.\n",
    "$$\\text{SV}=\\{\\vec{x}_i \\mid y_i(\\vec{w}\\cdot\\vec{x}_i+b)\\leq 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinge Loss\n",
    "$$\\ell(\\vec{x},y)=\\max\\big(0, 1-y(\\vec{w}\\cdot\\vec{x}+b)\\big)$$\n",
    "\n",
    "- if $\\vec{x}$ is incorrectly predicted, $\\ell(\\vec{x},y) > 1$\n",
    "- if $\\vec{x}$ is correctly predicted but within the margin, $1 \\geq \\ell(\\vec{x},y) > 0$\n",
    "- if $\\vec{x}$ is correctly predicted and far enough from the boundary, $\\ell(\\vec{x},y) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Risk\n",
    "$$L(\\vec{w},b)=\\frac{1}{2}\\|\\vec{w}\\|^2+\\frac{C}{N}\\sum_{i=1}^{N}\\ell(\\vec{x}_i,y_i)$$\n",
    "$$L_{\\text{SGD}}(\\vec{w},b)=\\frac{1}{2}\\|\\vec{w}\\|^2+C\\cdot\\ell(\\vec{x}_i,y_i)$$\n",
    "\n",
    "- $\\frac{1}{2}\\|\\vec{w}\\|^2$: regularization term; controls (increases) margin size\n",
    "- $C$: regularization term; manages trade off between margin size and classification error\n",
    "- $\\frac{1}{N}\\sum_{i=1}^{N}\\ell(\\vec{x}_i,y_i)$: classification error; sum of losses for all data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradients\n",
    "$$\\frac{\\partial \\ell}{\\partial \\vec{w}} = \\begin{cases}0&\\vec{x}\\not\\in\\text{SV}\\\\-y\\cdot\\vec{x}&\\vec{x}\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L}{\\partial \\vec{w}}=\\vec{w}+\\frac{C}{N}\\sum_{i=1}^{N}\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-y_i\\cdot\\vec{x}_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L_{\\text{SGD}}}{\\partial \\vec{w}}=\\vec{w}+\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-C\\cdot y_i\\cdot\\vec{x}_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial \\ell}{\\partial b} = \\begin{cases}0&\\vec{x}\\not\\in\\text{SV}\\\\-y&\\vec{x}\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L}{\\partial b}=\\frac{C}{N}\\sum_{i=1}^{N}\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-y_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$\n",
    "$$\\frac{\\partial L_{\\text{SGD}}}{\\partial b}=\\begin{cases}0&\\vec{x}_i\\not\\in\\text{SV}\\\\-C\\cdot y_i&\\vec{x}_i\\in\\text{SV}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update\n",
    "We will use Stochastic Gradient Descent (SGD)\n",
    "\n",
    "If $\\vec{x}$ is a support vector:\n",
    "$$\\vec{w} \\gets \\vec{w} - \\alpha\\cdot\\vec{w}+\\bigg(\\alpha\\cdot C\\cdot y_i\\cdot \\vec{x}_i\\bigg)$$\n",
    "$$b \\gets b + \\bigg(\\alpha\\cdot C\\cdot y_i\\bigg)$$\n",
    "\n",
    "If $\\vec{x}$ is not a support vector:\n",
    "$$\\vec{w} \\gets \\vec{w} - \\alpha\\cdot\\vec{w}$$\n",
    "$$b \\gets b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kernel SVM**\n",
    "\n",
    "Similar to linear SVM in a way that we want to find a hyperplane that separates data points into two classes with large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Function\n",
    "We use kernels to map training data from original sample space to higher dimensional space where data points are possibly linearly separable.\n",
    "\n",
    "Tranforming a data point $\\vec{x}$ into a high dimensional $\\phi(\\vec{x})$ is computationally inefficient.\n",
    "\n",
    "Valid kernel calculates $K(\\vec{x}, \\vec{x}')= \\phi(\\vec{x})\\cdot\\phi(\\vec{x}')$ for some mapping $\\phi$ without explicitly calculating $\\phi(\\vec{x})$ and $\\phi(\\vec{x}')$.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Linear Kernel**: $$K(\\vec{x},\\vec{x}')=\\vec{x}\\cdot\\vec{x}'$$\n",
    "\n",
    "**Polynomial Kernel**: $$K(\\vec{x},\\vec{x}')=(\\vec{x}\\cdot\\vec{x}'+c)^d$$\n",
    "where\n",
    "- $c$: constant, usually $0$ or $1$\n",
    "- $d$: degree of polynomial\n",
    "\n",
    "**Gaussian Kernel**: $$K(\\vec{x},\\vec{x}')=\\exp(-\\gamma\\|\\vec{x}-\\vec{x}'\\|^2)$$\n",
    "where \n",
    "- $\\gamma>0$: similarity sensitivity\n",
    "\n",
    "**Sigmoid Kernel**: $$K(\\vec{x},\\vec{x}')=\\tanh(\\alpha\\vec{x}\\cdot\\vec{x}'+c)$$\n",
    "where\n",
    "- $\\alpha, c$: hyperparameter constants (kernel is only valid for certain values)\n",
    "\n",
    "**Laplacian Kernel**: $$K(\\vec{x},\\vec{x}')=\\exp(-\\gamma\\|\\vec{x}-\\vec{x}'\\|_1)$$\n",
    "- $\\gamma>0$: similarity sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slack Variable\n",
    "\n",
    "$\\xi_i$ is a slack variable that measures how badly an $i$-th point violates the margin condition.\n",
    "We constrain $\\xi$ to following conditions:\n",
    "$$y_i(\\vec{w}\\cdot\\vec{x}_i+b)\\geq 1-\\xi_i \\quad \\text{with} \\quad \\xi\\geq 0$$\n",
    "\n",
    "- If $\\xi_i = 0$, $i$-th point is correctly classified and outside the margin.\n",
    "- If $0 < \\xi_i < 1$, $i$-th point is correctly classified but inside the margin.\n",
    "- If $1 \\leq \\xi_i$, $i$-th point is misclassified.\n",
    "\n",
    "We make $\\xi$ a free variable with constraints and leave optimization to find the exact value of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "For optimization, we want to \n",
    "- maximize the margin size ($\\mu=\\frac{1}{\\|\\vec{w}\\|}$) or  minimize the inverse of the margin ($\\|\\vec{w}\\|$) and\n",
    "- minimize the error ($\\xi$)\n",
    "\n",
    "So the problem turns into:\n",
    "$$\\min_{\\vec{w}, b, \\vec{\\xi}} \\bigg( \\frac{1}{2}\\|\\vec{w}\\|^2+C\\sum_{i=1}^{n}\\xi_i \\bigg)$$\n",
    "with constrains $y_i(\\vec{w}\\cdot\\vec{x}_i+b)\\geq 1-\\xi_i$ and $\\xi\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lagrangian Function\n",
    "If we want to minimize $f(\\vec{x})$ under the constraint $g_i(\\vec{x})\\geq 0$, then we use Lagrangian function:\n",
    "$$\\mathcal{L}(\\vec{x},\\vec{\\lambda})=f(\\vec{x})-\\sum_i\\lambda_i g_i(\\vec{x})$$\n",
    "where $\\lambda_i\\geq 0$ are Lagrangian multipliers.\n",
    "\n",
    "Then, the problem turns into:\n",
    "$$\\min_{\\vec{x}}\\max_{\\vec{\\lambda}}\\mathcal{L}(\\vec{x},\\vec{\\lambda})$$\n",
    "because\n",
    "- for fixed $\\lambda$: we minimize over $\\vec{x}$ to reduce cost and honor the constraint\n",
    "- for fixed $\\vec{x}$: we maximize over $\\lambda$ to penalize any violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KKT Conditions\n",
    "To ensure the solution actually satisfies all the original constraints and behaves correctly, we need Karush-Kuhn-Tucker (KKT) conditions.\n",
    "\n",
    "To find the optimal point $\\vec{x}^{\\ast}$ and multipliers $\\lambda_i^{\\ast}$ the followings must hold:\n",
    "\n",
    "1. Stationary: $$\\nabla_{\\vec{x}}\\mathcal{L}(\\vec{x}^{\\ast},\\vec{\\lambda}^{\\ast})=0\\quad \\text{which is} \\quad \\nabla f(\\vec{x}^{\\ast})=\\sum_i\\lambda_i^{\\ast}\\nabla g_i(\\vec{x}^{\\ast})$$\n",
    "At the optimal point $\\vec{x}^{\\ast}$, the gradient of the objective is a linear combination of the gradients of the active constraints, meaning we canâ€™t move in any direction that improves the objective without violating at least one constraint (reducing $f(\\vec{x})$ to $f(\\vec{x}')$ will also reduce $g_i(\\vec{x})=0$ to $g_i(\\vec{x}')<0$ for some $i$).\n",
    "\n",
    "2. Primal Feasibility $$g_i(\\vec{x}^{\\ast})\\geq 0$$\n",
    "The point $\\vec{x}^{\\ast}$ must satisfy the original constraint\n",
    "\n",
    "3. Dual Feasibility $$\\lambda_i^{\\ast}\\geq 0$$\n",
    "Lagrange multipliers are non-negative to penalize the violation of constraints $g_i(\\vec{x}) \\geq 0$.\n",
    "\n",
    "4. Complementary Slackness $$\\lambda_i^{\\ast} \\cdot g_i(\\vec{x}^{\\ast}) = 0$$\n",
    "Constraints only influence the optimization when they are active or tight ($g_i(\\vec{x})=0$ and $\\lambda_i>0$). Otherwise, they are ignored ($g_i(\\vec{x})>0$ and $\\lambda_i=0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization with Lagrangian Functions\n",
    "We will use Lagrangian function for the objective and the contraints above:\n",
    "$$\\mathcal{L}(\\vec{w},b,\\vec{\\xi},\\vec{\\alpha},\\vec{\\beta}) = \\frac{1}{2}\\|\\vec{w}\\|^2 + C\\sum_{i=1}^{n}\\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(\\vec{w}\\cdot\\vec{x}_i+b)-1+\\xi_i] - \\sum_{i=1}^{n}\\beta_i\\xi_i$$\n",
    "And the optimization problem is:\n",
    "$$\\min_{\\vec{w},b,\\vec{\\xi}}\\max_{\\vec{\\alpha},\\vec{\\beta}}\\mathcal{L}(\\vec{w},b,\\vec{\\xi},\\vec{\\alpha},\\vec{\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By applying KKT conditions, we get:\n",
    "1. Stationary:\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial\\vec{w}}=\\vec{w}-\\sum_i\\alpha_iy_i\\vec{x}_i=0 \\quad \\Rightarrow \\quad \\vec{w}=\\sum_i\\alpha_iy_i\\vec{x}_i$$\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial b} = -\\sum_i\\alpha_iy_i=0 \\quad \\Rightarrow \\quad \\sum_i\\alpha_iy_i=0$$\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial \\xi_i}=C-\\alpha_i-\\beta_i=0 \\quad \\Rightarrow \\quad \\alpha_i+\\beta_i=C \\quad \\text{and} \\quad \\alpha_i,\\beta_i\\leq C$$\n",
    "\n",
    "2. Primal Feasibility:\n",
    "$$y_i(\\vec{w}\\cdot\\vec{x}_i+b)\\geq 1-\\xi_i$$\n",
    "$$\\xi_i\\geq 0$$\n",
    "\n",
    "3. Dual Feasibility:\n",
    "$$\\alpha_i\\geq 0$$ \n",
    "$$\\beta_i\\geq 0$$\n",
    "\n",
    "4. Complementary Slackness:\n",
    "$$\\alpha_i\\cdot [y_i(\\vec{w}\\cdot\\vec{x}_i+b)-1+\\xi_i]=0$$\n",
    "$$\\beta_i\\cdot\\xi_i=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructing Optimization Problem\n",
    "Now we can rewrite the Lagrangian function:\n",
    "$$\\mathcal{L}(\\vec{w},b,\\vec{\\xi},\\vec{\\alpha},\\vec{\\beta}) = \\frac{1}{2}\\|\\vec{w}\\|^2 + C\\sum_{i=1}^{n}\\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(\\vec{w}\\cdot\\vec{x}_i+b)-1+\\xi_i] - \\sum_{i=1}^{n}\\beta_i\\xi_i$$\n",
    "With $\\frac{\\partial\\mathcal{L}}{\\partial\\xi_i}=0$, we can rewrite as:\n",
    "$$\\mathcal{L} = \\frac{1}{2}\\|\\vec{w}\\|^2 + \\sum_{i=1}^{n}(\\alpha_i+\\beta_i)\\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(\\vec{w}\\cdot\\vec{x}_i+b)-1+\\xi_i] - \\sum_{i=1}^{n}\\beta_i\\xi_i$$\n",
    "$$=\\frac{1}{2}\\|\\vec{w}\\|^2 - \\sum_{i=1}^{n}\\alpha_iy_i(\\vec{w}\\cdot\\vec{x}_i) - \\sum_{i=1}^{n}\\alpha_iy_ib + \\sum_{i=1}^{n}\\alpha_i$$\n",
    "\n",
    "Notice that:\n",
    "- $\\frac{1}{2}\\|\\vec{w}\\|^2 = \\frac{1}{2}\\sum\\limits_i\\sum\\limits_j\\alpha_i\\alpha_jy_iy_j(\\vec{x}_i\\cdot\\vec{x}_j)$ by $\\frac{\\partial\\mathcal{L}}{\\partial\\vec{w}}=0$\n",
    "- $\\sum\\limits_{i=1}^{n}\\alpha_iy_i(\\vec{w}\\cdot\\vec{x}_i) = \\sum\\limits_i\\sum\\limits_j\\alpha_i\\alpha_jy_iy_j(\\vec{x}_i\\cdot\\vec{x}_j)$ by $\\frac{\\partial\\mathcal{L}}{\\partial\\vec{w}}=0$ \n",
    "- $\\sum\\limits_{i=1}^{n}\\alpha_iy_ib = 0$ by $\\frac{\\partial\\mathcal{L}}{\\partial b}=0$\n",
    "\n",
    "So, the function can be further reduced to:\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{n}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j(\\vec{x}_i\\cdot\\vec{x}_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual SVM Optimization Problem\n",
    "Since this eliminates all variable except for $\\vec{\\alpha}$, we now have this optimization problem:\n",
    "$$\\max_{\\vec{\\alpha}}\\mathcal{L}(\\vec{\\alpha}) \\quad \\text{where} \\quad \\mathcal{L}(\\vec{\\alpha})=\\sum_{i=1}^{n}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j(\\vec{x}_i\\cdot\\vec{x}_j)$$\n",
    "\n",
    "subject to:\n",
    "- $0\\leq \\alpha_i \\leq C$\n",
    "- $\\sum_i \\alpha_i y_i = 0$\n",
    "\n",
    "We will find $\\alpha_i$ values for each data point by solving this problem.\n",
    "- $\\alpha_i=0$ if the data point is not a support vector\n",
    "- $0 < \\alpha_i < C$ if the data point is correctly classified but is a support vector\n",
    "- $\\alpha_i = C$ if the data point is misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual SVM Optimization with Kernels\n",
    "To classify not linearly separable data points, we use mapping $\\phi:\\mathbb{R}^d\\rightarrow \\mathbb{R}^{d'}$ from the original sample space to higher dimensional space and use SVM with $\\phi(x_i)$ as inputs.\n",
    "\n",
    "$$ \\min_{\\vec{w},b}L \\quad \\text{where} \\quad L(\\vec{w},b)=\\frac{1}{2}\\|\\vec{w}\\|^2+\\frac{C}{N}\\sum_{i=1}^{N}\\ell(\\phi(\\vec{x}_i),y_i)$$\n",
    "\n",
    "However, we know that it is inefficient to compute $\\phi(\\vec{x})$ directly, while the dot product $\\phi(\\vec{x})\\cdot \\phi(\\vec{x}')$ can be easily found by using some kernel $K(x,x')$.\n",
    "\n",
    "In dual SVM optimization problem, we only use the dot product of two data points to calculate alpha values, so we can utilize kernels to solve the problem for not linearly separable data points efficiently.\n",
    "\n",
    "$$\\max_{\\vec{\\alpha}}\\mathcal{L} \\quad \\text{where} \\quad \\mathcal{L}(\\vec{\\alpha})=\\sum_{i=1}^{n}\\alpha_i - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_i\\alpha_jy_iy_j K(\\vec{x}_i,\\vec{x}_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving Alpha\n",
    "Suppose $\\vec{\\alpha}=\\begin{bmatrix}\\alpha_1\\\\\\vdots\\\\\\alpha_n\\end{bmatrix}$, $\\vec{y}=\\begin{bmatrix}y_1\\\\\\vdots\\\\ y_n\\end{bmatrix}$, and $K$ be an $n\\times n$ matrix with entries $K_{ij}=K(\\vec{x}_i,\\vec{x}_j)$.\n",
    "Then the optimization problem is:\n",
    "$$\\max_{\\vec{\\alpha}}\\left(\\vec{1}^T\\vec{\\alpha}-\\frac{1}{2}\\vec{\\alpha}^TQ\\vec{\\alpha}\\right)$$\n",
    "where \n",
    "$$Q=\\text{diag}(\\vec{y})\\cdot K\\cdot \\text{diag}(\\vec{y})$$\n",
    "subject to:\n",
    "$$\\vec{y}^T\\vec{\\alpha}=0 \\quad \\text{and} \\quad 0 \\leq \\alpha_i \\leq C\\quad \\forall i$$\n",
    "\n",
    "Now, this is in a form of Quadratic Programming (QP) problem and can be solved with another Lagrangian function with KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary\n",
    "The decision boundary is $\\vec{w}\\cdot\\vec{x}+b=0$.\n",
    "\n",
    "However, by solving the dual form, we only know $\\vec{\\alpha}$, so we should replace $\\vec{w}$ and $b$ in terms of $\\alpha$.\n",
    "\n",
    "- For $\\vec{w}$:\\\n",
    "we had a condition $\\frac{\\partial\\mathcal{L}}{\\partial\\vec{w}}=0$, leading to \n",
    "$$\\vec{w}=\\sum_i\\alpha_iy_i\\vec{x}_i$$\n",
    "\n",
    "- For $b$:\\\n",
    "Suppose we choose the data point $k$ that is at the margin.\\\n",
    "Then, $y_k(\\vec{w}\\cdot\\vec{x}_k+b)=1$ and $b = y_k-\\vec{w}\\cdot\\vec{x}_k$ because $y=0$ or $1$. \\\n",
    "By substituting $\\vec{w}$ we found, we get\n",
    "$$b=y_k-\\sum_i\\alpha_iy_i(\\vec{x}_i\\cdot\\vec{x}_k)$$\n",
    "\n",
    "For stability of $b$, we can average value of $b$ computed with every support vector at the margin.\n",
    "$$b=\\frac{1}{|\\mathcal{S}|}\\sum_{k\\in\\mathcal{S}}\\left(y_k-\\sum_i\\alpha_iy_i(\\vec{x}_i\\cdot\\vec{x}_k)\\right)$$\n",
    "where $\\mathcal{S} = \\{k \\mid 0<\\alpha_k<C\\}$.\n",
    "\n",
    "In conclusion, the decision boundary is:\n",
    "$$\\sum_i\\alpha_iy_i(\\vec{x}_i\\cdot\\vec{x})+\\frac{1}{|\\mathcal{S}|}\\sum_{k\\in\\mathcal{S}}\\left(y_k-\\sum_i\\alpha_iy_i(\\vec{x}_i\\cdot\\vec{x}_k)\\right)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundary with Kernels\n",
    "If we use $\\phi(\\vec{x})$ instead of $\\vec{x}$, we can substitute $K(\\vec{x},\\vec{x}')$ to replace $\\phi(\\vec{x})\\cdot\\phi(\\vec{x}')$.\n",
    "\n",
    "So, the decision boundary would be:\n",
    "$$\\sum_i\\alpha_iy_i K(\\vec{x}_i,\\vec{x})+b=0$$\n",
    "\n",
    "and the predicted label for data point $\\vec{x}$ is:\n",
    "$$\\hat{y}=\\text{sign}\\left(\\sum_i\\alpha_iy_i K(\\vec{x}_i,\\vec{x})+b\\right)$$\n",
    "\n",
    "where \n",
    "$$b=\\frac{1}{|\\mathcal{S}|}\\sum_{k\\in\\mathcal{S}}\\left(y_k-\\sum_i\\alpha_iy_iK(\\vec{x}_i,\\vec{x}_k)\\right)$$\n",
    "and\n",
    "$$\\mathcal{S} = \\{k \\mid 0<\\alpha_k<C\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only SVM classifiers (binary) will be implemented.\n",
    "\n",
    "QP problem solver will not be implemented. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

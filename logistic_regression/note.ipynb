{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Geeks for Geeks - Logistic Regression in Machine Learning](https://www.geeksforgeeks.org/understanding-logistic-regression/)\n",
    "\n",
    "[Scikit Learn - LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[StatQuest: Logistic Regression](https://youtu.be/yIYKR4sgzI8?si=k2EGU-u74p-jecQW)\n",
    "\n",
    "[Google Developer Program](https://developers.google.com/machine-learning/crash-course/logistic-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics\n",
    "\n",
    "- Supervised learning algorithm for classification tasks.\n",
    "- Uses a inear model with a sigmoid activation function to map inputs to probabilities.\n",
    "- Outputs probability values between 0 and 1.\n",
    "- Can be extended to multiclass classification using softmax (multinomial logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "\n",
    "1. **Linearity in log odds**: The relationship between input features and the log odds of the target variable is linear.\n",
    "2. **Independence of observations**: Each training example is independent of others.\n",
    "3. **No extreme multicollinearity**: Features should not be highly correlated.\n",
    "4. **Sufficiently large dataset**: Logistic regression works best when there are enough data points per class.\n",
    "5. **Features should be scaled**: Standardizing features improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs & Outputs\n",
    "\n",
    "- **Input**: Feature matrix $X$ of shape $(n_{\\text{samples}}, n_{\\text{features}})$.\n",
    "- **Output**: Probability $\\hat{y}$ of shape $(n_{\\text{samples}},)$\n",
    "    - converted to a class ($0$ or $1$) using a decision threshold (usually $0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "- **Trainable Parameters**:\n",
    "  - $\\vec{w}$ (weights): $(n_{\\text{features}},)$\n",
    "  - $b$ (bias): scalar\n",
    "\n",
    "* **Hyperparameters**:\n",
    "  - $\\alpha$ (learning rate)\n",
    "  - epochs (Number of iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime Complexity\n",
    "\n",
    "- **Training Complexity**: $O(n \\cdot d \\cdot T)$\n",
    "- **Inference Complexity**: $O(n \\cdot d)$\n",
    "* where\n",
    "    - $n$: number of samples\n",
    "    - $d$: number of features\n",
    "    - $T$: number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros & Cons\n",
    "\n",
    "- **advantages**:\n",
    "    - Simple, interpretable, and easy to implement.\n",
    "    - Outputs probability scores, not just class labels.\n",
    "    - Works well when the decision boundary is approximately linear.\n",
    "    - Computationally efficient for small and medium sized datasets.\n",
    "\n",
    "* **disadvantages**:\n",
    "    - Struggles with non linearly separable data.\n",
    "    - Sensitive to outliers.\n",
    "    - Cannot handle complex feature interactions without feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "- **Medical Diagnosis** (e.g., predicting cancer probability)\n",
    "- **Spam Detection** (classifying emails as spam or not)\n",
    "- **Credit Risk Assessment** (predicting loan default probability)\n",
    "- **Customer Churn Prediction** (identifying users likely to cancel a subscription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Equations\n",
    "\n",
    "**Linear Combination of Inputs**:\n",
    "$$z = b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n = b + \\vec{w}^T\\vec{x}$$\n",
    "\n",
    "**Sigmoid Activation Function**:\n",
    "$$\\hat{y} = \\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "**Class Decision with Threshold**:\n",
    "$$\\text{returns}\\, \\begin{cases}1&\\hat{y}\\geq0.5\\\\0&\\hat{y}<0.5\\end{cases}$$\n",
    "\n",
    "where:\n",
    "- $e^z$: odd, the ratio of the probability of favorable outcomes and that of unfavorable outcomes ($\\frac{p}{1-p}$)\n",
    "- $\\sigma$: probability ($p$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Binary Cross-Entropy Loss (Log Loss):\n",
    "$$\\ell(\\hat{y}_i,y_i) = -y_i\\log(\\hat{y}_i)-(1-y_i)\\log(1-\\hat{y}_i)$$\n",
    "$$J(\\vec{w},b)=-\\frac{1}{n}\\sum\\limits_{i=1}^{n}(y_i\\log(\\hat{y}_i)+(1-y_i)\\log(1-\\hat{y}_i))$$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the actual label (0 or 1),\n",
    "- $\\hat{y}_i$ is the predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial\\ell}{\\partial \\hat{y}} &= -\\frac{y_i}{\\hat{y}_i}+\\frac{1-y_i}{1-\\hat{y}_i}\\\\\n",
    "    &=\\frac{\\hat{y}_i-y_i}{\\hat{y}_i(1-\\hat{y}_i)}\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial \\hat{y}}{\\partial z} &= \\frac{e^{-z}}{(1+e^{-z})^2}\\\\\n",
    "    &= \\frac{1}{1+e^{-z}}\\cdot\\left(1-\\frac{1}{1+e^{-z}}\\right)\\\\\n",
    "    &= \\hat{y}_i\\left(1-\\hat{y}_i\\right)\\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial z}{\\partial \\vec{w}} &= \\vec{x}\\\\\n",
    "    \\frac{\\partial z}{\\partial b} &= 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradients\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial \\vec{w}}&=\\frac{1}{n}\\sum\\frac{\\partial\\ell}{\\partial \\vec{w}}\\\\\n",
    "    &=\\frac{1}{n}\\sum\\left(\\frac{\\partial\\ell}{\\partial \\hat{y}}\\cdot\\frac{\\partial \\hat{y}}{\\partial z}\\cdot\\frac{\\partial z}{\\partial \\vec{w}}\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum\\limits(\\hat{y}-y)\\vec{x}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial b}&=\\frac{1}{n}\\sum\\frac{\\partial\\ell}{\\partial b}\\\\\n",
    "    &=\\frac{1}{n}\\sum\\left(\\frac{\\partial\\ell}{\\partial \\hat{y}}\\cdot\\frac{\\partial \\hat{y}}{\\partial z}\\cdot\\frac{\\partial z}{\\partial b}\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum\\limits(\\hat{y}-y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates\n",
    "\n",
    "$$\\vec{w}=\\vec{w}-\\alpha\\left(\\frac{1}{n}\\sum(y-\\hat{y})\\vec{x}\\right)$$\n",
    "$$b=b-\\alpha\\left(\\frac{1}{n}\\sum(y-\\hat{y})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although multinomial logistic regression exists, only binomial model will be implemented."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

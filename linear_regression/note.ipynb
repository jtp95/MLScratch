{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Geeks for Geeks - Linear Regression in Machine Learning](https://www.geeksforgeeks.org/ml-linear-regression/)\n",
    "\n",
    "[Scikit Learn - LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Characteristics**\n",
    "\n",
    "- supervised learning\n",
    "- computes linear relationship between target (dependent variable) and features (independent variables)\n",
    "- finds line of best fit\n",
    "- assumes several conditions\n",
    "    - linearity: there exists linear relationship between IV and DV\n",
    "    - independece: each observation is independent of each other\n",
    "    - homoscedasticity: variances of errors are constant for every IV\n",
    "    - normality: residuals are normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Equation**\n",
    "$$y=b+w_1x_1+w_2x_2+\\cdots+w_nx_n=b+\\vec{w}^T\\vec{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "Mean Squared Error:\n",
    "$$\\ell(\\hat{y}_i,y_i)=(\\hat{y}_i-y_i)^2$$\n",
    "$$J(\\vec{w},b)=\\frac{1}{n}\\sum\\limits_{i=1}^{n}(\\hat{y}_i-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial \\vec{w}}&=\\frac{\\partial}{\\partial \\vec{w}}\\left(\\frac{1}{n}\\sum(\\hat{y}-y)^2\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum(2(\\hat{y}-y)\\cdot\\frac{\\partial \\hat{y}}{\\partial\\vec{w}})\\\\\n",
    "    &=\\frac{2}{n}\\sum(\\hat{y}-y)\\vec{x}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial b}&=\\frac{\\partial}{\\partial b}\\left(\\frac{1}{n}\\sum(\\hat{y}-y)^2\\right)\\\\\n",
    "    &=\\frac{1}{n}\\sum(2(\\hat{y}-y)\\cdot\\frac{\\partial\\hat{y}}{\\partial b})\\\\\n",
    "    &=\\frac{2}{n}\\sum(\\hat{y}-y)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Update**\n",
    "\n",
    "$$\\vec{w}=\\vec{w}-\\alpha\\left(\\frac{2}{n}\\sum(\\hat{y}-y)\\vec{x}\\right)$$\n",
    "$$b=b-\\alpha\\left(\\frac{2}{n}\\sum(\\hat{y}-y)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal solution to the linear regression problem can be found by using normal equation:\n",
    "$$\\vec{w}=(X^TX)^{-1}X^T\\vec{y}.$$\n",
    "However, it is slower and computationally more expensive as the data gets larger."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
